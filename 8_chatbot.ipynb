{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Rule-based chat bot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "raw_html = urllib.request.urlopen('https://en.wikipedia.org/wiki/Manchester_United_F.C.')\n",
    "raw_html = raw_html.read()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "article_html = bs.BeautifulSoup(raw_html, 'html')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "article_para = article_html.find_all('p')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "article_text = ''\n",
    "for para in article_para:\n",
    "    article_text += para.text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "article_text = article_text.lower()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
    "article_text = re.sub(r'\\s+', ' ', article_text)\n",
    "\n",
    "article_sentences = nltk.sent_tokenize(article_text)\n",
    "article_words = nltk.word_tokenize(article_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "wnlemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def perform_lemmatization(tokens):\n",
    "    return [wnlemmatizer.lemmatize(token) for token in tokens]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "punctuation_removal = dict((ord(punct), None) for punct in string.punctuation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def get_processed_text(document):\n",
    "    return perform_lemmatization(nltk.word_tokenize(document.lower().translate(punctuation_removal)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "greeting_inputs = (\"hey\", \"good morning\", \"good evening\", \"morning\", \"evening\", \"hi\", \"whatsup\")\n",
    "greeting_responses = [\"hey\", \"hey hows you?\", \"*nods*\", \"hello, how you doing\", \"hello\", \"Welcome, I am good and you\"]\n",
    "\n",
    "\n",
    "def generate_greeting_response(greeting):\n",
    "    for token in greeting.split():\n",
    "        if token.lower() in greeting_inputs:\n",
    "            return random.choice(greeting_responses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "following method generates a response to user queries. takes user input, finds cosine similarity of user input and compare with sentences in corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def generate_response(user_input):\n",
    "    response = ''\n",
    "    article_sentences.append(user_input)\n",
    "\n",
    "    word_vectorizer = TfidfVectorizer(tokenizer=get_processed_text, stop_words='english')\n",
    "    all_word_vectorizer = word_vectorizer.fit_transform(article_sentences)\n",
    "\n",
    "    # finds cosine similarity between the last item and word vector for all the sentences in corpus.\n",
    "    similar_vector_values = cosine_similarity(all_word_vectorizer[-1], all_word_vectorizer)\n",
    "\n",
    "    # sort list containing all cosine similarities of vectors, the second last item in the list will have the highest cosine(after sorting) with user input. the last item is the user input itself, so we dont select that\n",
    "    similar_sentence_number = similar_vector_values.argsort()[0][-2]\n",
    "\n",
    "    # flatten the retrieved cosine similarity and check if the similarity is equal to zero or not.\n",
    "    matched_vector = similar_vector_values.flatten()\n",
    "    matched_vector.sort()\n",
    "    vector_matched = matched_vector[-2]\n",
    "\n",
    "    if vector_matched == 0:\n",
    "        response = response + 'cant understand shit ya cunt'\n",
    "        return response\n",
    "    else:\n",
    "        response = response + article_sentences[similar_sentence_number]\n",
    "        return response"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Mfers. Ask me:\n",
      "TennisRobo: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/PycharmProjects/stackabuse/venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cant understand shit ya cunt\n",
      "TennisRobo: the rivalry with arsenal arises from the numerous times the two teams, as well as managers alex ferguson and arsène wenger, have battled for the premier league title.\n",
      "TennisRobo: wayne rooney scored his 250th goal for united, a stoppage time equaliser in a league game against stoke city in january, surpassing sir bobby charlton as the club's all-time top scorer.\n",
      "TennisRobo: the rivalry with arsenal arises from the numerous times the two teams, as well as managers alex ferguson and arsène wenger, have battled for the premier league title.\n",
      "TennisRobo: wayne rooney scored his 250th goal for united, a stoppage time equaliser in a league game against stoke city in january, surpassing sir bobby charlton as the club's all-time top scorer.\n",
      "TennisRobo: van gaal was ultimately sacked just two days after the cup final victory, with united having finished 5th in the league.\n",
      "Adios\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cont_dialogue = True\n",
    "print('Hello Mfers. Ask me:')\n",
    "while cont_dialogue:\n",
    "    human_text = input()\n",
    "    human_text = human_text.lower()\n",
    "    if human_text != 'bye':\n",
    "        if human_text == 'thanks' or human_text == 'thank you very much' or human_text == 'thank you':\n",
    "            cont_dialogue = False\n",
    "            print('Adios')\n",
    "        else:\n",
    "            if generate_greeting_response(human_text) is not None:\n",
    "                print('Bot:' + generate_greeting_response(human_text))\n",
    "            else:\n",
    "                print(\"TennisRobo: \", end=\"\")\n",
    "                print(generate_response(human_text))\n",
    "                article_sentences.remove(human_text)\n",
    "    else:\n",
    "        cont_dialogue = False\n",
    "        print('Adiois')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}